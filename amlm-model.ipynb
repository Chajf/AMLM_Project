{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-20T11:32:00.896948Z","iopub.status.busy":"2024-06-20T11:32:00.896094Z","iopub.status.idle":"2024-06-20T11:32:07.887615Z","shell.execute_reply":"2024-06-20T11:32:07.886825Z","shell.execute_reply.started":"2024-06-20T11:32:00.896913Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from transformers import BertTokenizer, BertModel, BertConfig\n","import torch.optim as optim\n","from torch.nn import MSELoss, TransformerEncoder, TransformerEncoderLayer\n","import pandas as pd\n","import numpy as np\n","import os\n","import time\n","import re\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader, Dataset\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.nn.utils.rnn import pad_sequence"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-06-20T11:32:07.890337Z","iopub.status.busy":"2024-06-20T11:32:07.889339Z","iopub.status.idle":"2024-06-20T11:32:44.773272Z","shell.execute_reply":"2024-06-20T11:32:44.772361Z","shell.execute_reply.started":"2024-06-20T11:32:07.890299Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ytid</th>\n","      <th>start_s</th>\n","      <th>end_s</th>\n","      <th>audioset_positive_labels</th>\n","      <th>aspect_list</th>\n","      <th>caption</th>\n","      <th>author_id</th>\n","      <th>is_balanced_subset</th>\n","      <th>is_audioset_eval</th>\n","      <th>audio_features</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-0Gj8-vB1q4</td>\n","      <td>30</td>\n","      <td>40</td>\n","      <td>/m/0140xf,/m/02cjck,/m/04rlf</td>\n","      <td>['low quality', 'sustained strings melody', 's...</td>\n","      <td>The low quality recording features a ballad so...</td>\n","      <td>4</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>[[-40.99979, -40.858463, -44.591995, -42.415, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0SdAVK79lg</td>\n","      <td>30</td>\n","      <td>40</td>\n","      <td>/m/0155w,/m/01lyv,/m/0342h,/m/042v_gx,/m/04rlf...</td>\n","      <td>['guitar song', 'piano backing', 'simple percu...</td>\n","      <td>This song features an electric guitar as the m...</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>[[-47.59354, -44.512863, -42.904636, -44.10178...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0vPFx-wRRI</td>\n","      <td>30</td>\n","      <td>40</td>\n","      <td>/m/025_jnm,/m/04rlf</td>\n","      <td>['amateur recording', 'finger snipping', 'male...</td>\n","      <td>a male voice is singing a melody with changing...</td>\n","      <td>6</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>[[-47.81186, -51.93988, -60.75714, -58.41188, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-0xzrMun0Rs</td>\n","      <td>30</td>\n","      <td>40</td>\n","      <td>/m/01g90h,/m/04rlf</td>\n","      <td>['backing track', 'jazzy', 'digital drums', 'p...</td>\n","      <td>This song contains digital drums playing a sim...</td>\n","      <td>6</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>[[-25.13749, -30.922548, -43.063854, -38.35595...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-1LrH01Ei1w</td>\n","      <td>30</td>\n","      <td>40</td>\n","      <td>/m/02p0sh1,/m/04rlf</td>\n","      <td>['rubab instrument', 'repetitive melody on dif...</td>\n","      <td>This song features a rubber instrument being p...</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>[[-22.071985, -19.954773, -20.680244, -20.6276...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5516</th>\n","      <td>zw5dkiklbhE</td>\n","      <td>15</td>\n","      <td>25</td>\n","      <td>/m/01sm1g,/m/0l14md</td>\n","      <td>['amateur recording', 'percussion', 'wooden bo...</td>\n","      <td>This audio contains someone playing a wooden b...</td>\n","      <td>6</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>[[-42.779514, -41.359673, -48.218506, -50.4325...</td>\n","    </tr>\n","    <tr>\n","      <th>5517</th>\n","      <td>zwfo7wnXdjs</td>\n","      <td>30</td>\n","      <td>40</td>\n","      <td>/m/02p0sh1,/m/04rlf,/m/06j64v</td>\n","      <td>['instrumental music', 'arabic music', 'genera...</td>\n","      <td>The song is an instrumental. The song is mediu...</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>[[-38.654037, -40.134087, -46.6978, -51.105743...</td>\n","    </tr>\n","    <tr>\n","      <th>5518</th>\n","      <td>zx_vcwOsDO4</td>\n","      <td>50</td>\n","      <td>60</td>\n","      <td>/m/01glhc,/m/02sgy,/m/0342h,/m/03lty,/m/04rlf,...</td>\n","      <td>['instrumental', 'no voice', 'electric guitar'...</td>\n","      <td>The rock music is purely instrumental and feat...</td>\n","      <td>2</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>[[-48.891968, -56.01191, -55.617645, -58.16963...</td>\n","    </tr>\n","    <tr>\n","      <th>5519</th>\n","      <td>zyXa2tdBTGc</td>\n","      <td>30</td>\n","      <td>40</td>\n","      <td>/m/04rlf,/t/dd00034</td>\n","      <td>['instrumental music', 'gospel music', 'strong...</td>\n","      <td>The song is an instrumental. The song is slow ...</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>[[-47.926243, -47.799667, -54.788925, -57.6263...</td>\n","    </tr>\n","    <tr>\n","      <th>5520</th>\n","      <td>zzNdwF40ID8</td>\n","      <td>70</td>\n","      <td>80</td>\n","      <td>/m/04rlf,/m/0790c</td>\n","      <td>['glitch', 'noise', 'instrumental', 'electroni...</td>\n","      <td>This is a glitch music piece. There is a synth...</td>\n","      <td>9</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>[[-43.288086, -43.02362, -50.565575, -62.20537...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5419 rows Ã— 10 columns</p>\n","</div>"],"text/plain":["             ytid  start_s  end_s  \\\n","0     -0Gj8-vB1q4       30     40   \n","1     -0SdAVK79lg       30     40   \n","2     -0vPFx-wRRI       30     40   \n","3     -0xzrMun0Rs       30     40   \n","4     -1LrH01Ei1w       30     40   \n","...           ...      ...    ...   \n","5516  zw5dkiklbhE       15     25   \n","5517  zwfo7wnXdjs       30     40   \n","5518  zx_vcwOsDO4       50     60   \n","5519  zyXa2tdBTGc       30     40   \n","5520  zzNdwF40ID8       70     80   \n","\n","                               audioset_positive_labels  \\\n","0                          /m/0140xf,/m/02cjck,/m/04rlf   \n","1     /m/0155w,/m/01lyv,/m/0342h,/m/042v_gx,/m/04rlf...   \n","2                                   /m/025_jnm,/m/04rlf   \n","3                                    /m/01g90h,/m/04rlf   \n","4                                   /m/02p0sh1,/m/04rlf   \n","...                                                 ...   \n","5516                                /m/01sm1g,/m/0l14md   \n","5517                      /m/02p0sh1,/m/04rlf,/m/06j64v   \n","5518  /m/01glhc,/m/02sgy,/m/0342h,/m/03lty,/m/04rlf,...   \n","5519                                /m/04rlf,/t/dd00034   \n","5520                                  /m/04rlf,/m/0790c   \n","\n","                                            aspect_list  \\\n","0     ['low quality', 'sustained strings melody', 's...   \n","1     ['guitar song', 'piano backing', 'simple percu...   \n","2     ['amateur recording', 'finger snipping', 'male...   \n","3     ['backing track', 'jazzy', 'digital drums', 'p...   \n","4     ['rubab instrument', 'repetitive melody on dif...   \n","...                                                 ...   \n","5516  ['amateur recording', 'percussion', 'wooden bo...   \n","5517  ['instrumental music', 'arabic music', 'genera...   \n","5518  ['instrumental', 'no voice', 'electric guitar'...   \n","5519  ['instrumental music', 'gospel music', 'strong...   \n","5520  ['glitch', 'noise', 'instrumental', 'electroni...   \n","\n","                                                caption  author_id  \\\n","0     The low quality recording features a ballad so...          4   \n","1     This song features an electric guitar as the m...          0   \n","2     a male voice is singing a melody with changing...          6   \n","3     This song contains digital drums playing a sim...          6   \n","4     This song features a rubber instrument being p...          0   \n","...                                                 ...        ...   \n","5516  This audio contains someone playing a wooden b...          6   \n","5517  The song is an instrumental. The song is mediu...          1   \n","5518  The rock music is purely instrumental and feat...          2   \n","5519  The song is an instrumental. The song is slow ...          1   \n","5520  This is a glitch music piece. There is a synth...          9   \n","\n","      is_balanced_subset  is_audioset_eval  \\\n","0                  False              True   \n","1                  False             False   \n","2                  False              True   \n","3                  False              True   \n","4                  False             False   \n","...                  ...               ...   \n","5516               False             False   \n","5517                True              True   \n","5518                True              True   \n","5519               False             False   \n","5520                True              True   \n","\n","                                         audio_features  \n","0     [[-40.99979, -40.858463, -44.591995, -42.415, ...  \n","1     [[-47.59354, -44.512863, -42.904636, -44.10178...  \n","2     [[-47.81186, -51.93988, -60.75714, -58.41188, ...  \n","3     [[-25.13749, -30.922548, -43.063854, -38.35595...  \n","4     [[-22.071985, -19.954773, -20.680244, -20.6276...  \n","...                                                 ...  \n","5516  [[-42.779514, -41.359673, -48.218506, -50.4325...  \n","5517  [[-38.654037, -40.134087, -46.6978, -51.105743...  \n","5518  [[-48.891968, -56.01191, -55.617645, -58.16963...  \n","5519  [[-47.926243, -47.799667, -54.788925, -57.6263...  \n","5520  [[-43.288086, -43.02362, -50.565575, -62.20537...  \n","\n","[5419 rows x 10 columns]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["data = pd.read_pickle(\"/kaggle/input/audio-dataset-mel-256/audio_dataset_256.pkl\")\n","data"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-20T11:32:44.774876Z","iopub.status.busy":"2024-06-20T11:32:44.774518Z","iopub.status.idle":"2024-06-20T11:32:44.785682Z","shell.execute_reply":"2024-06-20T11:32:44.784668Z","shell.execute_reply.started":"2024-06-20T11:32:44.774846Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(256, 861)"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["data['audio_features'].iloc[0].shape"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-06-20T11:32:44.788846Z","iopub.status.busy":"2024-06-20T11:32:44.788195Z","iopub.status.idle":"2024-06-20T11:32:44.798445Z","shell.execute_reply":"2024-06-20T11:32:44.797395Z","shell.execute_reply.started":"2024-06-20T11:32:44.788820Z"},"trusted":true},"outputs":[{"data":{"text/plain":["array([[-40.99979 , -40.858463, -44.591995, ..., -38.19406 , -36.710896,\n","        -38.839684],\n","       [-46.70161 , -42.733616, -40.621758, ..., -37.159702, -33.01973 ,\n","        -35.885857],\n","       [-44.044918, -45.685513, -45.924263, ..., -37.338154, -36.525146,\n","        -38.146904],\n","       ...,\n","       [-79.06895 , -77.472626, -77.350655, ..., -69.1514  , -75.54788 ,\n","        -75.59643 ],\n","       [-80.      , -80.      , -80.      , ..., -79.809616, -80.      ,\n","        -80.      ],\n","       [-80.      , -80.      , -80.      , ..., -80.      , -80.      ,\n","        -80.      ]], dtype=float32)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["data['audio_features'].iloc[0]"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-20T11:32:44.800041Z","iopub.status.busy":"2024-06-20T11:32:44.799717Z","iopub.status.idle":"2024-06-20T11:32:44.814608Z","shell.execute_reply":"2024-06-20T11:32:44.813690Z","shell.execute_reply.started":"2024-06-20T11:32:44.800009Z"},"trusted":true},"outputs":[],"source":["def clip_array(arr):\n","    return arr[:, :420]\n","\n","# Apply the clipping function to each array in the 'audio_features' column\n","data['audio_features'] = data['audio_features'].apply(clip_array)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-06-20T11:32:44.967322Z","iopub.status.busy":"2024-06-20T11:32:44.966965Z","iopub.status.idle":"2024-06-20T11:32:44.982268Z","shell.execute_reply":"2024-06-20T11:32:44.981471Z","shell.execute_reply.started":"2024-06-20T11:32:44.967292Z"},"trusted":true},"outputs":[],"source":["# lstm - TS task\n","\n","class MusicGenerationModel(nn.Module):\n","    def __init__(self, bert_model_name='bert-base-uncased', hidden_dim=512, lstm_hidden_dim=256, output_dim=256, n_lstm_layers=2, dropout=0.1):\n","        super(MusicGenerationModel, self).__init__()\n","        self.tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n","        self.text_encoder = BertModel.from_pretrained(bert_model_name)\n","        \n","        self.fc_text_to_lstm = nn.Linear(self.text_encoder.config.hidden_size, lstm_hidden_dim)\n","        self.lstm = nn.LSTM(lstm_hidden_dim, lstm_hidden_dim, n_lstm_layers, batch_first=True, dropout=dropout)\n","        self.fc_lstm_to_output = nn.Linear(lstm_hidden_dim, output_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, text_description, seq_length=420):\n","        # Tokenize and encode the input text description\n","        encoded_input = self.tokenizer(text_description, return_tensors='pt', padding=True, truncation=True).to(next(self.parameters()).device)\n","        with torch.no_grad():\n","            text_embeddings = self.text_encoder(**encoded_input).last_hidden_state\n","\n","        # Use the [CLS] token's embedding as the representation of the entire sequence\n","        cls_embedding = text_embeddings[:, 0, :]  # Extract the [CLS] token embedding\n","        \n","        # Map the text embedding to the LSTM hidden dimension size\n","        lstm_input = self.fc_text_to_lstm(cls_embedding).unsqueeze(1)  # Shape: (batch_size, 1, lstm_hidden_dim)\n","        \n","        # Initialize hidden and cell states for the LSTM\n","        h_0 = torch.zeros(self.lstm.num_layers, lstm_input.size(0), self.lstm.hidden_size).to(next(self.parameters()).device)\n","        c_0 = torch.zeros(self.lstm.num_layers, lstm_input.size(0), self.lstm.hidden_size).to(next(self.parameters()).device)\n","        \n","        # Sequentially generate each value in the spectrogram\n","        outputs = []\n","        lstm_output, (h_t, c_t) = self.lstm(lstm_input, (h_0, c_0))\n","        for _ in range(seq_length):\n","            lstm_output, (h_t, c_t) = self.lstm(lstm_output, (h_t, c_t))\n","            output = self.fc_lstm_to_output(lstm_output)\n","            outputs.append(output)\n","            lstm_output = output  # Feed the current output as the next input\n","        \n","        audio_output = torch.cat(outputs, dim=1).squeeze(2)  # Concatenate all outputs along the sequence length dimension\n","        audio_output = audio_output.squeeze(0)  # Remove the first dimension of size 1, resulting in shape [431, 256]\n","        audio_output = audio_output.transpose(0, 1)\n","        \n","        return audio_output"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-06-20T11:32:44.984141Z","iopub.status.busy":"2024-06-20T11:32:44.983365Z","iopub.status.idle":"2024-06-20T11:32:49.200273Z","shell.execute_reply":"2024-06-20T11:32:49.199325Z","shell.execute_reply.started":"2024-06-20T11:32:44.984109Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b08a920cf64b41af9526a84f0786a4a3","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a080022436bf4113b58620629a300855","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e5f7fdd5927f43448b1465c40c961928","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6b2a47cd2a0c4415aab59bd892ff751d","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4fe2879adeff408ab0c274d3d7490440","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = MusicGenerationModel().to(device)"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-06-20T11:36:34.970440Z","iopub.status.busy":"2024-06-20T11:36:34.969626Z","iopub.status.idle":"2024-06-20T11:37:34.256595Z","shell.execute_reply":"2024-06-20T11:37:34.255651Z","shell.execute_reply.started":"2024-06-20T11:36:34.970394Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","Epoch 1, Training Loss: 5.6290473841190884, Validation Loss: 21.23659039599429, Time Elapsed: 6.14 seconds\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","Epoch 2, Training Loss: 5.628806904195502, Validation Loss: 21.23564761679111, Time Elapsed: 5.41 seconds\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","Epoch 3, Training Loss: 5.628572239155313, Validation Loss: 21.234749431539726, Time Elapsed: 6.35 seconds\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","Epoch 4, Training Loss: 5.628352582990917, Validation Loss: 21.23393322708862, Time Elapsed: 6.11 seconds\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","Epoch 5, Training Loss: 5.628158833328828, Validation Loss: 21.233234475899447, Time Elapsed: 5.90 seconds\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","Epoch 6, Training Loss: 5.627995214055291, Validation Loss: 21.232680091998674, Time Elapsed: 5.62 seconds\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","Epoch 7, Training Loss: 5.627872087771229, Validation Loss: 21.232285728313826, Time Elapsed: 6.07 seconds\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","Epoch 8, Training Loss: 5.627787356613141, Validation Loss: 21.232045416462466, Time Elapsed: 6.12 seconds\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","Epoch 9, Training Loss: 5.62774057003316, Validation Loss: 21.231934269415937, Time Elapsed: 6.06 seconds\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","torch.Size([256, 420])\n","Epoch 10, Training Loss: 5.627721111997189, Validation Loss: 21.23190532839166, Time Elapsed: 5.47 seconds\n"]}],"source":["num_epochs = 50  # Set the number of epochs\n","training_losses = []\n","validation_losses = []\n","\n","# Define loss and optimizer\n","criterion = nn.MSELoss()\n","# optimizer = optim.Adam(model.parameters(), lr=1e-5)\n","optimizer = optim.Adam(model.parameters(), lr=1e-5)\n","# scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2)\n","scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n","\n","\n","# Directory to save model checkpoints and losses\n","checkpoint_dir = './checkpoints'\n","os.makedirs(checkpoint_dir, exist_ok=True)\n","\n","# Initialize best loss to a large value\n","best_loss = float('inf')\n","early_stopping_patience = 5\n","early_stopping_counter = 0\n","\n","# Split your data into training and validation sets\n","train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)  # Adjust test_size and random_state as needed\n","\n","\n","for epoch in range(num_epochs):\n","    epoch_start_time = time.time()\n","    epoch_loss = 0.0\n","    \n","    # Training phase\n","    model.train()\n","    for i, row in train_data.iterrows():\n","#         text_description = ''.join(row['aspect_list'])\n","        text_description = row['caption']\n","#         print(text_description)\n","        # Ensure audio_target is a 2D tensor\n","        audio_target = torch.tensor(row['audio_features']).to(device)\n","#         audio_target = pad_audio_features(row['audio_features'], 861).to(device)\n","#         audio_target = torch.tensor(row['normalized_audio_features']).unsqueeze(0).to(device)\n","        if len(audio_target.shape) == 3:\n","            audio_target = audio_target.squeeze(0).permute(1, 0)\n","        \n","        seq_length = audio_target.shape[0]\n","        \n","        optimizer.zero_grad()\n","#         output = model([text_description], seq_length)\n","        output = model([text_description])\n","        \n","        if output.shape != audio_target.shape:\n","            print(f\"Output shape {output.shape} and target shape {audio_target.shape} do not match!\")\n","            continue\n","\n","        loss = criterion(output, audio_target)   \n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        optimizer.step()\n","        \n","        epoch_loss += loss.item()\n","    \n","    avg_epoch_loss = epoch_loss / len(train_data)\n","    training_losses.append(avg_epoch_loss)\n","    \n","    # Validation phase\n","    model.eval()\n","    val_loss = 0.0\n","    with torch.no_grad():\n","        for i, row in val_data.iterrows():\n","#             text_description = ''.join(row['aspect_list'])\n","            text_description = row['caption']\n","            # Ensure audio_target is a 2D tensor\n","            audio_target = torch.tensor(row['audio_features']).to(device)\n","#             audio_target = pad_audio_features(row['audio_features'], 861).to(device)\n","#             audio_target = torch.tensor(row['normalized_audio_features']).unsqueeze(0).to(device)\n","            \n","            if len(audio_target.shape) == 3:\n","                audio_target = audio_target.squeeze(0).permute(1, 0)\n","        \n","            seq_length = audio_target.shape[0]\n","            \n","#             output = model([text_description], seq_length)\n","            output = model([text_description])\n","            \n","            if output.shape != audio_target.shape:\n","                print(f\"Output shape {output.shape} and target shape {audio_target.shape} do not match!\")\n","                continue\n","\n","            loss = criterion(output, audio_target)\n","            val_loss += loss.item()\n","    \n","    avg_val_loss = val_loss / len(val_data)\n","    validation_losses.append(avg_val_loss)\n","    \n","#     scheduler.step(avg_val_loss) #if cosine we dont need that\n","    scheduler.step()\n","    \n","    # Early Stopping check\n","    if avg_val_loss < best_loss:\n","        best_loss = avg_val_loss\n","        early_stopping_counter = 0\n","        \n","        checkpoint_path = os.path.join(checkpoint_dir, 'best_model.pth')\n","        torch.save({\n","            'epoch': epoch + 1,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': avg_val_loss,\n","        }, checkpoint_path)\n","        \n","    else:\n","        early_stopping_counter += 1\n","        if early_stopping_counter >= early_stopping_patience:\n","            print(\"Early stopping triggered.\")\n","            break\n","    \n","    # Save training and validation losses to files\n","    loss_file = os.path.join(checkpoint_dir, 'training_losses.txt')\n","    with open(loss_file, 'a') as f:\n","        f.write(f\"Epoch {epoch+1}, Training Loss: {avg_epoch_loss}, Validation Loss: {avg_val_loss}\\n\")\n","    \n","    # Save validation losses to a separate file\n","    val_loss_file = os.path.join(checkpoint_dir, 'validation_losses.txt')\n","    with open(val_loss_file, 'a') as f:\n","        f.write(f\"Epoch {epoch+1}, Validation Loss: {avg_val_loss}\\n\")\n","    \n","    epoch_end_time = time.time()\n","    epoch_duration = epoch_end_time - epoch_start_time\n","    \n","    print(f\"Epoch {epoch+1}, Training Loss: {avg_epoch_loss}, Validation Loss: {avg_val_loss}, Time Elapsed: {epoch_duration:.2f} seconds\")\n","\n","# Save the final training and validation losses to files\n","final_loss_file = os.path.join(checkpoint_dir, 'final_losses.txt')\n","with open(final_loss_file, 'w') as f:\n","    for epoch, (train_loss, val_loss) in enumerate(zip(training_losses, validation_losses), 1):\n","        f.write(f\"Epoch {epoch}, Training Loss: {train_loss}, Validation Loss: {val_loss}\\n\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5226418,"sourceId":8712013,"sourceType":"datasetVersion"},{"datasetId":5231663,"sourceId":8719164,"sourceType":"datasetVersion"}],"dockerImageVersionId":30732,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
